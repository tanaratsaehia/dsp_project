{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\combine_all_class_with_label_no_fft.csv'\n",
    "# file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\combine_all_class_with_label_apply_fft.csv'\n",
    "# file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\cleaned_combine_all_class_with_label_applied_fft.csv'\n",
    "file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win5_lab50.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.84      0.72      0.78        53\n",
      "          descending_stairs       0.90      0.88      0.89        73\n",
      "                    nothing       1.00      0.93      0.96        54\n",
      "                    running       0.98      1.00      0.99        53\n",
      "sitting_standing_transition       0.93      0.98      0.95        64\n",
      "                    walking       0.86      0.97      0.91        67\n",
      "\n",
      "                   accuracy                           0.91       364\n",
      "                  macro avg       0.92      0.91      0.91       364\n",
      "               weighted avg       0.92      0.91      0.91       364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Prepare for modeling --\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "#    - We'll drop 'start_time' because it's just a reference\n",
    "#    - We'll keep 'label_encoded' as the target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,   # 20% for testing\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# Apply min-max normalization (0-1) to these columns\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# 4) Train a Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5) Evaluate\n",
    "y_pred = model.predict(scaler.transform(X_test))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow onnx onnxmltools skl2onnx nyoka scikit-learn joblib pandas numpy onnxruntime jpmml-evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.85      0.78      0.82       170\n",
      "          descending_stairs       0.83      0.81      0.82       158\n",
      "                    nothing       1.00      0.91      0.95       148\n",
      "                    running       1.00      1.00      1.00       145\n",
      "sitting_standing_transition       0.91      0.99      0.94       146\n",
      "                    walking       0.85      0.94      0.89       154\n",
      "\n",
      "                   accuracy                           0.90       921\n",
      "                  macro avg       0.91      0.91      0.90       921\n",
      "               weighted avg       0.90      0.90      0.90       921\n",
      "\n",
      "Model, Scaler, and Label Encoder have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import onnxmltools\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from nyoka import skl_to_pmml\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -- Load Data --\n",
    "file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win2_lab50.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_test_raw = X_test\n",
    "X_train_raw = X_train\n",
    "# 4) Normalize features (0-1 scaling)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 5) Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# 7) Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "import joblib\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"rf_model.pkl\")\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "# Save the label encoder\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"Model, Scaler, and Label Encoder have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {0: 'climbing_stairs', 1: 'descending_stairs', 2: 'nothing', 3: 'running', 4: 'sitting_standing_transition', 5: 'walking'}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping encoded labels to original labels\n",
    "label_mapping = {index: label for index, label in enumerate(le.classes_)}\n",
    "\n",
    "# Print the mapping\n",
    "print(\"Label Mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USE TORCH\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class RF_NN(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(RF_NN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 160)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(160, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# # Create the PyTorch model\n",
    "# input_size = X_train.shape[1]\n",
    "# num_classes = len(set(y))\n",
    "# torch_model = RF_NN(input_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_57\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_57\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Normalization (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_329 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_330 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,590</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_331 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,970</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_332 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,610</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_333 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_334 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_335 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_336 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Normalization (\u001b[38;5;33mLambda\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_329 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m85\u001b[0m)             │         \u001b[38;5;34m4,420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_330 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)             │         \u001b[38;5;34m5,590\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_331 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m)             │         \u001b[38;5;34m2,970\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_332 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)             │         \u001b[38;5;34m1,610\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_333 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_334 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m390\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_335 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_336 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,106</span> (62.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,106\u001b[0m (62.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,106</span> (62.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,106\u001b[0m (62.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3752 - loss: 1.5995\n",
      "Epoch 2/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6526 - loss: 0.7866\n",
      "Epoch 3/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.5662\n",
      "Epoch 4/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.4898\n",
      "Epoch 5/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8175 - loss: 0.4494\n",
      "Epoch 6/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8332 - loss: 0.4219\n",
      "Epoch 7/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8391 - loss: 0.4010\n",
      "Epoch 8/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8454 - loss: 0.3847\n",
      "Epoch 9/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8572 - loss: 0.3695\n",
      "Epoch 10/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8669 - loss: 0.3517\n",
      "Epoch 11/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8701 - loss: 0.3413\n",
      "Epoch 12/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3309\n",
      "Epoch 13/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8792 - loss: 0.3207\n",
      "Epoch 14/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8850 - loss: 0.3087\n",
      "Epoch 15/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8846 - loss: 0.3015\n",
      "Epoch 16/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8904 - loss: 0.2926\n",
      "Epoch 17/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8906 - loss: 0.2849\n",
      "Epoch 18/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8955 - loss: 0.2746\n",
      "Epoch 19/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8887 - loss: 0.2965\n",
      "Epoch 20/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9035 - loss: 0.2611\n",
      "Epoch 21/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8971 - loss: 0.2673\n",
      "Epoch 22/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9071 - loss: 0.2573\n",
      "Epoch 23/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9061 - loss: 0.2562\n",
      "Epoch 24/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9085 - loss: 0.2436\n",
      "Epoch 25/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9230 - loss: 0.2186\n",
      "Epoch 26/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9288 - loss: 0.2034\n",
      "Epoch 27/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9305 - loss: 0.1963\n",
      "Epoch 28/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9309 - loss: 0.1938\n",
      "Epoch 29/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9350 - loss: 0.1858\n",
      "Epoch 30/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9310 - loss: 0.1879\n",
      "Epoch 31/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9324 - loss: 0.1822\n",
      "Epoch 32/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9357 - loss: 0.1854\n",
      "Epoch 33/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9374 - loss: 0.1785\n",
      "Epoch 34/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9312 - loss: 0.1878\n",
      "Epoch 35/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.1818\n",
      "Epoch 36/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9356 - loss: 0.1659\n",
      "Epoch 37/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9320 - loss: 0.1640\n",
      "Epoch 38/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9429 - loss: 0.1515\n",
      "Epoch 39/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9467 - loss: 0.1463\n",
      "Epoch 40/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9451 - loss: 0.1416\n",
      "Epoch 41/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.1373\n",
      "Epoch 42/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9482 - loss: 0.1345\n",
      "Epoch 43/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9516 - loss: 0.1278\n",
      "Epoch 44/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9499 - loss: 0.1262\n",
      "Epoch 45/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9512 - loss: 0.1247\n",
      "Epoch 46/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9488 - loss: 0.1257\n",
      "Epoch 47/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9489 - loss: 0.1251\n",
      "Epoch 48/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9525 - loss: 0.1253\n",
      "Epoch 49/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1283\n",
      "Epoch 50/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.1230\n",
      "Epoch 51/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9477 - loss: 0.1370\n",
      "Epoch 52/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9461 - loss: 0.1297\n",
      "Epoch 53/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9491 - loss: 0.1306\n",
      "Epoch 54/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.1378\n",
      "Epoch 55/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.1203\n",
      "Epoch 56/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9544 - loss: 0.1175\n",
      "Epoch 57/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9546 - loss: 0.1169\n",
      "Epoch 58/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9507 - loss: 0.1247\n",
      "Epoch 59/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9531 - loss: 0.1214\n",
      "Epoch 60/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.1233\n",
      "Epoch 61/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9605 - loss: 0.1028\n",
      "Epoch 62/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9618 - loss: 0.1000\n",
      "Epoch 63/63\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9559 - loss: 0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model saved to win2_lab50_acc93.keras\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# -- Load Data --\n",
    "file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win2_lab50.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_test_raw = X_test\n",
    "X_train_raw = X_train\n",
    "# 4) Normalize features (0-1 scaling)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels for MLP training\n",
    "num_classes = len(le.classes_)\n",
    "y_train_ohe = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_ohe = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Create constant tensors for the normalization parameters\n",
    "scaler_scale = tf.constant(scaler.scale_, dtype=tf.float32)\n",
    "scaler_min = tf.constant(scaler.min_, dtype=tf.float32)\n",
    "\n",
    "def rf_to_nn_with_normalization(activation_fn:str):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_raw.shape[1],)),\n",
    "        tf.keras.layers.Lambda(lambda x: x * scaler_scale + scaler_min, name=\"Normalization\"),\n",
    "        tf.keras.layers.Dense(85, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(65, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(45, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(35, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(25, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(15, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(10, activation=activation_fn,\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\",\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))\n",
    "    ])\n",
    "\n",
    "# Create and compile the model with normalization incorporated\n",
    "tf_model = rf_to_nn_with_normalization('relu')\n",
    "tf_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "tf_model.summary()\n",
    "\n",
    "# Train the model using one-hot encoded labels\n",
    "# Note: Since the model applies internal normalization, we feed raw features.\n",
    "tf_model.fit(\n",
    "    X_train_raw, \n",
    "    y_train_ohe,  \n",
    "    epochs=63, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the Keras model in the new .keras format\n",
    "keras_model_path = \"win2_lab50_acc93.keras\"\n",
    "tf_model.save(keras_model_path, save_format=\"keras\")\n",
    "\n",
    "print(f\"Keras model saved to {keras_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.85932518\n",
      "Iteration 2, loss = 1.75775949\n",
      "Iteration 3, loss = 1.59641079\n",
      "Iteration 4, loss = 1.48893446\n",
      "Iteration 5, loss = 1.38226889\n",
      "Iteration 6, loss = 1.28167120\n",
      "Iteration 7, loss = 1.18083430\n",
      "Iteration 8, loss = 1.05474521\n",
      "Iteration 9, loss = 0.86794332\n",
      "Iteration 10, loss = 0.66892382\n",
      "Iteration 11, loss = 0.57176436\n",
      "Iteration 12, loss = 0.54954714\n",
      "Iteration 13, loss = 0.53240748\n",
      "Iteration 14, loss = 0.51496153\n",
      "Iteration 15, loss = 0.49607306\n",
      "Iteration 16, loss = 0.48624464\n",
      "Iteration 17, loss = 0.48081737\n",
      "Iteration 18, loss = 0.46260065\n",
      "Iteration 19, loss = 0.44839538\n",
      "Iteration 20, loss = 0.42862581\n",
      "Iteration 21, loss = 0.41745226\n",
      "Iteration 22, loss = 0.40013400\n",
      "Iteration 23, loss = 0.38431289\n",
      "Iteration 24, loss = 0.37570049\n",
      "Iteration 25, loss = 0.39143806\n",
      "Iteration 26, loss = 0.41133117\n",
      "Iteration 27, loss = 0.35307296\n",
      "Iteration 28, loss = 0.35286919\n",
      "Iteration 29, loss = 0.34207319\n",
      "Iteration 30, loss = 0.33600760\n",
      "Iteration 31, loss = 0.32739526\n",
      "Iteration 32, loss = 0.32440323\n",
      "Iteration 33, loss = 0.32147532\n",
      "Iteration 34, loss = 0.31341950\n",
      "Iteration 35, loss = 0.31899900\n",
      "Iteration 36, loss = 0.31027417\n",
      "Iteration 37, loss = 0.30675844\n",
      "Iteration 38, loss = 0.30575497\n",
      "Iteration 39, loss = 0.30627578\n",
      "Iteration 40, loss = 0.29406307\n",
      "Iteration 41, loss = 0.29106367\n",
      "Iteration 42, loss = 0.29157979\n",
      "Iteration 43, loss = 0.28331312\n",
      "Iteration 44, loss = 0.28904030\n",
      "Iteration 45, loss = 0.28773431\n",
      "Iteration 46, loss = 0.28358342\n",
      "Iteration 47, loss = 0.27364945\n",
      "Iteration 48, loss = 0.27276258\n",
      "Iteration 49, loss = 0.26683950\n",
      "Iteration 50, loss = 0.26451720\n",
      "Iteration 51, loss = 0.26458911\n",
      "Iteration 52, loss = 0.26319945\n",
      "Iteration 53, loss = 0.26940625\n",
      "Iteration 54, loss = 0.26230665\n",
      "Iteration 55, loss = 0.26313380\n",
      "Iteration 56, loss = 0.25516494\n",
      "Iteration 57, loss = 0.25576794\n",
      "Iteration 58, loss = 0.27861123\n",
      "Iteration 59, loss = 0.26426458\n",
      "Iteration 60, loss = 0.24878218\n",
      "Iteration 61, loss = 0.24961423\n",
      "Iteration 62, loss = 0.24226928\n",
      "Iteration 63, loss = 0.23622624\n",
      "Iteration 64, loss = 0.23641249\n",
      "Iteration 65, loss = 0.23744488\n",
      "Iteration 66, loss = 0.23469008\n",
      "Iteration 67, loss = 0.23549701\n",
      "Iteration 68, loss = 0.23275639\n",
      "Iteration 69, loss = 0.23008421\n",
      "Iteration 70, loss = 0.22502526\n",
      "Iteration 71, loss = 0.22484076\n",
      "Iteration 72, loss = 0.23266379\n",
      "Iteration 73, loss = 0.22316583\n",
      "Iteration 74, loss = 0.22894209\n",
      "Iteration 75, loss = 0.22166141\n",
      "Iteration 76, loss = 0.21834119\n",
      "Iteration 77, loss = 0.22425323\n",
      "Iteration 78, loss = 0.22141252\n",
      "Iteration 79, loss = 0.21300108\n",
      "Iteration 80, loss = 0.21465708\n",
      "Iteration 81, loss = 0.21712765\n",
      "Iteration 82, loss = 0.21996918\n",
      "Iteration 83, loss = 0.21116409\n",
      "Iteration 84, loss = 0.20950848\n",
      "Iteration 85, loss = 0.21262601\n",
      "Iteration 86, loss = 0.20965192\n",
      "Iteration 87, loss = 0.20940560\n",
      "Iteration 88, loss = 0.20181398\n",
      "Iteration 89, loss = 0.19981292\n",
      "Iteration 90, loss = 0.20180303\n",
      "Iteration 91, loss = 0.20026762\n",
      "Iteration 92, loss = 0.21616029\n",
      "Iteration 93, loss = 0.23047086\n",
      "Iteration 94, loss = 0.21027840\n",
      "Iteration 95, loss = 0.20284867\n",
      "Iteration 96, loss = 0.19708011\n",
      "Iteration 97, loss = 0.19596094\n",
      "Iteration 98, loss = 0.19022209\n",
      "Iteration 99, loss = 0.18723589\n",
      "Iteration 100, loss = 0.18853417\n",
      "Iteration 101, loss = 0.19069456\n",
      "Iteration 102, loss = 0.19368458\n",
      "Iteration 103, loss = 0.18489254\n",
      "Iteration 104, loss = 0.18491080\n",
      "Iteration 105, loss = 0.18285552\n",
      "Iteration 106, loss = 0.18704915\n",
      "Iteration 107, loss = 0.18921834\n",
      "Iteration 108, loss = 0.18076197\n",
      "Iteration 109, loss = 0.18754129\n",
      "Iteration 110, loss = 0.18383173\n",
      "Iteration 111, loss = 0.18285250\n",
      "Iteration 112, loss = 0.18764443\n",
      "Iteration 113, loss = 0.18361346\n",
      "Iteration 114, loss = 0.18288060\n",
      "Iteration 115, loss = 0.17685797\n",
      "Iteration 116, loss = 0.17230633\n",
      "Iteration 117, loss = 0.17326311\n",
      "Iteration 118, loss = 0.17260307\n",
      "Iteration 119, loss = 0.17285059\n",
      "Iteration 120, loss = 0.17329360\n",
      "Iteration 121, loss = 0.17276509\n",
      "Iteration 122, loss = 0.16674722\n",
      "Iteration 123, loss = 0.17284605\n",
      "Iteration 124, loss = 0.17616824\n",
      "Iteration 125, loss = 0.18031026\n",
      "Iteration 126, loss = 0.17438457\n",
      "Iteration 127, loss = 0.16659370\n",
      "Iteration 128, loss = 0.16983081\n",
      "Iteration 129, loss = 0.16878336\n",
      "Iteration 130, loss = 0.16063989\n",
      "Iteration 131, loss = 0.16161502\n",
      "Iteration 132, loss = 0.15608933\n",
      "Iteration 133, loss = 0.16915959\n",
      "Iteration 134, loss = 0.15825969\n",
      "Iteration 135, loss = 0.15837448\n",
      "Iteration 136, loss = 0.15493027\n",
      "Iteration 137, loss = 0.17148695\n",
      "Iteration 138, loss = 0.15926448\n",
      "Iteration 139, loss = 0.15600537\n",
      "Iteration 140, loss = 0.15528853\n",
      "Iteration 141, loss = 0.15285230\n",
      "Iteration 142, loss = 0.15593050\n",
      "Iteration 143, loss = 0.16341495\n",
      "Iteration 144, loss = 0.15887620\n",
      "Iteration 145, loss = 0.15301870\n",
      "Iteration 146, loss = 0.15688161\n",
      "Iteration 147, loss = 0.15181096\n",
      "Iteration 148, loss = 0.15482977\n",
      "Iteration 149, loss = 0.15030167\n",
      "Iteration 150, loss = 0.15553480\n",
      "Iteration 151, loss = 0.16185928\n",
      "Iteration 152, loss = 0.15903724\n",
      "Iteration 153, loss = 0.14886420\n",
      "Iteration 154, loss = 0.14658568\n",
      "Iteration 155, loss = 0.13981226\n",
      "Iteration 156, loss = 0.14574223\n",
      "Iteration 157, loss = 0.13909280\n",
      "Iteration 158, loss = 0.13657895\n",
      "Iteration 159, loss = 0.13819836\n",
      "Iteration 160, loss = 0.13602277\n",
      "Iteration 161, loss = 0.14424482\n",
      "Iteration 162, loss = 0.13821796\n",
      "Iteration 163, loss = 0.13471242\n",
      "Iteration 164, loss = 0.13577522\n",
      "Iteration 165, loss = 0.14236140\n",
      "Iteration 166, loss = 0.13422226\n",
      "Iteration 167, loss = 0.13529998\n",
      "Iteration 168, loss = 0.14031414\n",
      "Iteration 169, loss = 0.17676327\n",
      "Iteration 170, loss = 0.20342514\n",
      "Iteration 171, loss = 0.15341401\n",
      "Iteration 172, loss = 0.13582790\n",
      "Iteration 173, loss = 0.13494211\n",
      "Iteration 174, loss = 0.13023992\n",
      "Iteration 175, loss = 0.13457178\n",
      "Iteration 176, loss = 0.13132934\n",
      "Iteration 177, loss = 0.13230155\n",
      "Iteration 178, loss = 0.12817928\n",
      "Iteration 179, loss = 0.12890951\n",
      "Iteration 180, loss = 0.12580797\n",
      "Iteration 181, loss = 0.13010665\n",
      "Iteration 182, loss = 0.12352166\n",
      "Iteration 183, loss = 0.12287513\n",
      "Iteration 184, loss = 0.11830645\n",
      "Iteration 185, loss = 0.11476549\n",
      "Iteration 186, loss = 0.11991673\n",
      "Iteration 187, loss = 0.11791658\n",
      "Iteration 188, loss = 0.13900647\n",
      "Iteration 189, loss = 0.12642600\n",
      "Iteration 190, loss = 0.11766258\n",
      "Iteration 191, loss = 0.11862347\n",
      "Iteration 192, loss = 0.11987876\n",
      "Iteration 193, loss = 0.12089919\n",
      "Iteration 194, loss = 0.11657767\n",
      "Iteration 195, loss = 0.11683259\n",
      "Iteration 196, loss = 0.11921937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training Accuracy: 0.9573\n",
      "Test Accuracy: 0.8990\n",
      "\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.83      0.89      0.86       170\n",
      "          descending_stairs       0.87      0.78      0.82       158\n",
      "                    nothing       0.97      0.93      0.95       148\n",
      "                    running       0.96      0.99      0.97       145\n",
      "sitting_standing_transition       0.87      0.95      0.91       146\n",
      "                    walking       0.92      0.87      0.90       154\n",
      "\n",
      "                   accuracy                           0.90       921\n",
      "                  macro avg       0.90      0.90      0.90       921\n",
      "               weighted avg       0.90      0.90      0.90       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -- Load Data --\n",
    "file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win2_lab50.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1).values\n",
    "y = df['label_encoded'].values\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# 4) Normalize features (0-1 scaling)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define MLPClassifier with custom hidden layers\n",
    "mlp_sklearn = MLPClassifier(\n",
    "    hidden_layer_sizes=(80, 60, 40, 30, 20, 15, 10),  # Custom architecture\n",
    "    activation='relu',  # Activation function\n",
    "    solver='adam',  # Optimizer (same as PyTorch version)\n",
    "    max_iter=500,  # Number of epochs\n",
    "    random_state=SEED,\n",
    "    verbose=True,\n",
    "    early_stopping=False,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp_sklearn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Training Accuracy: {mlp_sklearn.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {mlp_sklearn.score(X_test, y_test):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model Performance:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.85      0.89      0.87       170\n",
      "          descending_stairs       0.87      0.87      0.87       158\n",
      "                    nothing       1.00      0.93      0.96       148\n",
      "                    running       1.00      0.99      0.99       145\n",
      "sitting_standing_transition       0.91      0.97      0.94       146\n",
      "                    walking       0.94      0.92      0.93       154\n",
      "\n",
      "                   accuracy                           0.93       921\n",
      "                  macro avg       0.93      0.93      0.93       921\n",
      "               weighted avg       0.93      0.93      0.93       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ **Test TFLite Model**\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "tflite_model_path = \"random_forest.tflite\"\n",
    "interpreter = tflite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "tflite_preds = []\n",
    "for sample in X_test_raw.values.astype(np.float32):\n",
    "    sample = np.expand_dims(sample, axis=0)\n",
    "    interpreter.set_tensor(input_index, sample)\n",
    "    interpreter.invoke()\n",
    "    pred = interpreter.get_tensor(output_index)\n",
    "    tflite_preds.append(np.argmax(pred))\n",
    "\n",
    "print(\"TFLite Model Performance:\\n\", classification_report(y_test, tflite_preds, target_names=le.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0037419571351546255, -0.010255735861780687, -0.0005187563533817045, -0.0007779271309784636, -0.011684530606556422, -0.005450266280245839, 0.0010814544484847614, -0.010499645183615583, -0.010953739740245078, 0.0005338700387511322, -0.007200384902161859, -0.015312613124998927, -0.002975404036703231, -0.0031894352651102202, -0.01700029660480256, -0.00893347837083537, -0.0003689109478433577, -0.015278091894180038, -0.015786924220581665, -0.0005017439457106806, -0.010589392477812825, -0.021345068779959075, -0.004534786922936576, -0.004591712904678152, -0.023462354270661682, -0.012109227658250444, 0.0002218563385282049, -0.020801224793278724, -0.021437109942985158, 0.0012258380737344265, -0.013437896293417148, -0.029609449759968987, -0.003510306921295012, -0.003116930258726445, -0.033250760949093114, -0.01452654075837359, 0.0069118844267535475, -0.02927754466342892, -0.030638764282488667, 0.012323855887878953, -0.015334695717194022, -0.04909655126534018, 0.007814968676430655, 0.011056372443999752, -0.06621366057998368, -0.01583208993570091, 0.059790257572238714, -0.07830538231088299, -0.10952199139461402, 0.2867374282550782, 0.584, 0.2867374282550782, -0.10952199139461402, -0.07830538231088299, 0.059790257572238714, -0.01583208993570091, -0.06621366057998368, 0.011056372443999752, 0.007814968676430655, -0.04909655126534018, -0.015334695717194022, 0.012323855887878953, -0.030638764282488667, -0.02927754466342892, 0.0069118844267535475, -0.01452654075837359, -0.033250760949093114, -0.003116930258726445, -0.003510306921295012, -0.029609449759968987, -0.013437896293417148, 0.0012258380737344265, -0.021437109942985158, -0.020801224793278724, 0.0002218563385282049, -0.012109227658250444, -0.023462354270661682, -0.004591712904678152, -0.004534786922936576, -0.021345068779959075, -0.010589392477812825, -0.0005017439457106806, -0.015786924220581665, -0.015278091894180038, -0.0003689109478433577, -0.00893347837083537, -0.01700029660480256, -0.0031894352651102202, -0.002975404036703231, -0.015312613124998927, -0.007200384902161859, 0.0005338700387511322, -0.010953739740245078, -0.010499645183615583, 0.0010814544484847614, -0.005450266280245839, -0.011684530606556422, -0.0007779271309784636, -0.0005187563533817045, -0.010255735861780687, -0.0037419571351546255]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([np.float64(-2.4572991291393254), np.float64(-2.938421045617125),\n",
       "       np.float64(-3.5433295666435347), np.float64(-2.9608317019068386),\n",
       "       np.float64(-3.005196457714609), np.float64(-3.902374115708245),\n",
       "       np.float64(-3.5705299697139714), np.float64(-3.0938659492508944),\n",
       "       np.float64(-4.0438263501567215), np.float64(-4.234147669793814),\n",
       "       np.float64(-3.3393399535995556), np.float64(-3.958773310232863),\n",
       "       np.float64(-4.821588823467398), np.float64(-3.830029707736495),\n",
       "       np.float64(-3.733336910151764), np.float64(-5.162674625299965),\n",
       "       np.float64(-4.595645164817652), np.float64(-3.524083143938681),\n",
       "       np.float64(-5.087626916533736), np.float64(-5.580962821200157),\n",
       "       np.float64(-3.5832988776013037), np.float64(-4.410217255507737),\n",
       "       np.float64(-6.6099571012472795), np.float64(-4.461708871545995),\n",
       "       np.float64(-2.562844875156558), np.float64(-4.055568889156495),\n",
       "       np.float64(1.1721884624435246), np.float64(13.932130311833223),\n",
       "       np.float64(18.267387270466468), np.float64(12.465289338056227),\n",
       "       np.float64(12.13862058349167), np.float64(17.810882873544177),\n",
       "       np.float64(18.118450994028812), np.float64(16.706301368114005),\n",
       "       np.float64(20.0206401176808), np.float64(18.789694042925994),\n",
       "       np.float64(9.197320382504323), np.float64(3.9622591608282924),\n",
       "       np.float64(6.822612097598063), np.float64(7.349413492736976),\n",
       "       np.float64(3.86712573082472), np.float64(2.623916685344809),\n",
       "       np.float64(1.8869029717080075), np.float64(-1.2138226651217447),\n",
       "       np.float64(-2.0749190244650046), np.float64(0.7852947817960175),\n",
       "       np.float64(2.4072715539202023), np.float64(1.6050094945202855),\n",
       "       np.float64(0.7841865716939013), np.float64(-0.3913786289772796),\n",
       "       np.float64(-2.0461325027962873), np.float64(-2.106753191946154),\n",
       "       np.float64(-0.9289972940875328), np.float64(-0.8561231834506197),\n",
       "       np.float64(-2.0132035734535196), np.float64(-3.1571938943783637),\n",
       "       np.float64(-3.700612193142803), np.float64(-3.516808061322377),\n",
       "       np.float64(-3.3073327491758095), np.float64(-3.7646480129308846),\n",
       "       np.float64(-3.989933748447582), np.float64(-3.457394327279655),\n",
       "       np.float64(-3.3378350160988677), np.float64(-3.7006897927492357),\n",
       "       np.float64(-3.419491362596781), np.float64(-3.026158039131738),\n",
       "       np.float64(-3.303559636721903), np.float64(-3.293091509918605),\n",
       "       np.float64(-2.7967757960226614), np.float64(-2.862136363884812),\n",
       "       np.float64(-3.060506715132191), np.float64(-2.6263327482464787),\n",
       "       np.float64(-2.432325279377241), np.float64(-2.6936322532543158),\n",
       "       np.float64(-2.438146174617939), np.float64(-2.103309981162269),\n",
       "       np.float64(-2.450164395530512), np.float64(-2.5280386757205466),\n",
       "       np.float64(-1.9560295864872714), np.float64(-2.01541031044134),\n",
       "       np.float64(-1.9060754410664136), np.float64(-1.551815149986291),\n",
       "       np.float64(-1.5343157649213788), np.float64(-1.4173624793212234),\n",
       "       np.float64(-1.133337907088584), np.float64(-1.0722544569788897),\n",
       "       np.float64(-0.9768042591898863), np.float64(-0.7117382463381304),\n",
       "       np.float64(-0.7212122148976766), np.float64(-0.7205486481841302),\n",
       "       np.float64(-0.5426078616514567), np.float64(-0.4517553365046057),\n",
       "       np.float64(-0.5078467139264334), np.float64(-0.398225103983662),\n",
       "       np.float64(-0.29971555043742804), np.float64(-0.3625987325531597),\n",
       "       np.float64(-0.32124720119699857), np.float64(-0.22432600822765011),\n",
       "       np.float64(-0.21337934626397784), np.float64(-0.20221965650810236),\n",
       "       np.float64(-0.14161987098902545)], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_lpf_coefficients(N, wc):\n",
    "    midpoint = N / 2\n",
    "    h = []\n",
    "    for n in range(N + 1):\n",
    "        if n == midpoint:\n",
    "            #add wc/pi\n",
    "            h.append(wc/np.pi)\n",
    "        else:\n",
    "            #add FIR coefficient\n",
    "            result = (wc/np.pi) * (np.sin(wc*(n-(N/2)))/(wc*(n-(N/2))))\n",
    "            h.append(result)\n",
    "    return np.array(h)\n",
    "    \n",
    "# Parameters\n",
    "h_fqu_cut = 15   # Cutoff frequency (Hz)\n",
    "l_feq_cut = 0.4   # Cutoff frequency (Hz)\n",
    "fs = 50 # sampling rate on Hz\n",
    "N = 100      # Filter order\n",
    "wc_h = 2*np.pi*(h_fqu_cut/fs) # Normalized cutoff angular frequency\n",
    "wc_l = 2*np.pi*(l_feq_cut/fs) # Normalized cutoff angular frequency\n",
    "\n",
    "# Compute the BPF coefficients\n",
    "h_lpf_150 = compute_lpf_coefficients(N, wc_h)\n",
    "h_lpf_100 = compute_lpf_coefficients(N, wc_l)\n",
    "h_bpf = h_lpf_150-h_lpf_100\n",
    "print(h_bpf.tolist())\n",
    "\n",
    "example_signal = fft_df.iloc[1, 2:].values\n",
    "filtered_signal = np.convolve(example_signal, h_bpf, mode='same')  # Convolve with HPF coefficients\n",
    "filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
