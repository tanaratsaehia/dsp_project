{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import os\n",
    "\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def cleaning101(data):\n",
    "    df = data.copy()\n",
    "    x = np.array(df['x'].values)\n",
    "    y = np.array(df['y'].values)\n",
    "    z = np.array(df['z'].values)\n",
    "    magnitude = calculate_magnitude(x, y, z)\n",
    "    df['magnitude'] = magnitude\n",
    "\n",
    "    fs = 50  # Sampling frequency\n",
    "    lowcut = 0.4  # Lower cutoff frequency (Hz)\n",
    "    highcut = 15  # Upper cutoff frequency (Hz)\n",
    "    signal = np.array(df['magnitude'])\n",
    "\n",
    "    filtered_data = bandpass_filter(signal, lowcut, highcut, fs)\n",
    "    df['filtered_magnitude'] = filtered_data\n",
    "    # df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "def segment_and_flatten_magnitude(label, df, magnitude_column_name='filtered_magnitude', window_size_sec=5, overlap=0.5, sampling_rate=50):\n",
    "    \"\"\"\n",
    "    1) Combine x, y, z signals into a single magnitude signal.\n",
    "    2) Segment the magnitude signal into windows of window_size_sec seconds \n",
    "    with the specified overlap.\n",
    "    3) Flatten each window into columns: start_time, x1, x2, ..., xN \n",
    "    (where N = window_size_sec * sampling_rate).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing columns 'time', 'x', 'y', 'z'.\n",
    "                        'time' is in milliseconds.\n",
    "        magnitude_column_name (string): Target column name that already calculate magnitude.\n",
    "        window_size_sec (float): Length of each window in seconds (default 5).\n",
    "        overlap (float): Fraction of window overlap (default 0.5 = 50%).\n",
    "        sampling_rate (int): Sampling rate in Hz (default 50).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Each row corresponds to a flattened window.\n",
    "                    Columns:\n",
    "                    - 'start_time': millisecond timestamp of the window start\n",
    "                    - 'x1', 'x2', ..., 'xN': magnitude values for each sample\n",
    "    \"\"\"\n",
    "    # Sort by time to ensure chronological order\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "    # 2) Calculate number of samples per window and step size\n",
    "    window_samples = int(window_size_sec * sampling_rate)  # e.g., 5s * 50Hz = 250\n",
    "    step_size = int(window_samples * (1 - overlap))        # e.g., 250 * 0.5 = 125\n",
    "\n",
    "    flattened_rows = []\n",
    "\n",
    "    # 3) Loop through data with the given step size\n",
    "    for start_idx in range(0, len(df) - window_samples + 1, step_size):\n",
    "        # Extract this window of magnitude values\n",
    "        window = df.iloc[start_idx : start_idx + window_samples].reset_index(drop=True)\n",
    "        \n",
    "        # Prepare a dict for one flattened row\n",
    "        row_dict = {}\n",
    "        row_dict['start_time'] = window.loc[0, 'time']\n",
    "        row_dict['label'] = label\n",
    "        \n",
    "        # Flatten magnitude into x1, x2, x3, ..., xN\n",
    "        for i in range(window_samples):\n",
    "            row_dict[f'x{i+1}'] = window.loc[i, magnitude_column_name]\n",
    "        \n",
    "        flattened_rows.append(row_dict)\n",
    "\n",
    "    # Convert list of dicts into a DataFrame\n",
    "    return pd.DataFrame(flattened_rows)\n",
    "\n",
    "def compute_fft_on_flattened_data(df, num_samples=250, remove_flatten=True):\n",
    "    \"\"\"\n",
    "    Compute the FFT of the time-series in columns x1...xN, take the magnitude spectrum, \n",
    "    and keep only the first half (non-redundant part).\n",
    "    \n",
    "    The returned DataFrame contains:\n",
    "    - 'start_time' and 'label'\n",
    "    - Original data (x1-x250) if remove_flatten=False\n",
    "    - FFT features starting from x251 (fft1-fft126)\n",
    "    \"\"\"\n",
    "    feature_columns = [f'x{i+1}' for i in range(num_samples)]\n",
    "    fft_features_list = []\n",
    "\n",
    "    # Loop through each row to compute FFT\n",
    "    for _, row in df.iterrows():\n",
    "        ts = row[feature_columns].values.astype(np.float64)\n",
    "        fft_vals = np.fft.fft(ts)\n",
    "        fft_mag = np.abs(fft_vals)\n",
    "        half_fft = fft_mag[:num_samples // 2 + 1]  # Keep first half (+1 for DC component)\n",
    "        fft_features_list.append(half_fft)\n",
    "\n",
    "    # Create FFT feature DataFrame\n",
    "    num_fft_features = num_samples // 2 + 1\n",
    "    fft_df = pd.DataFrame(\n",
    "        fft_features_list, \n",
    "        columns=[f'x{i+1}' for i in range(num_samples+1, num_samples+1+num_fft_features)]\n",
    "    )\n",
    "\n",
    "    # Include original data if required\n",
    "    if not remove_flatten:\n",
    "        combined_df = pd.concat([df[['start_time', 'label'] + feature_columns].reset_index(drop=True), fft_df], axis=1)\n",
    "    else:\n",
    "        combined_df = pd.concat([df[['start_time', 'label']].reset_index(drop=True), fft_df], axis=1)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (5966, 102)\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.84      0.88      0.86       161\n",
      "          descending_stairs       0.93      0.75      0.83       149\n",
      "                    nothing       0.99      0.93      0.96       253\n",
      "                    running       1.00      1.00      1.00       213\n",
      "sitting_standing_transition       0.91      0.98      0.94       208\n",
      "                    walking       0.88      0.96      0.92       210\n",
      "\n",
      "                   accuracy                           0.93      1194\n",
      "                  macro avg       0.92      0.92      0.92      1194\n",
      "               weighted avg       0.93      0.93      0.93      1194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1 clean and flatten data\n",
    "root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\cleaned_data_many_position\"\n",
    "# root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\clean_datasets\"\n",
    "magnitude_column_name = \"filtered_magnitude\"  # Adjust if needed\n",
    "# magnitude_column_name = \"magnitude\"  # raw magnitude\n",
    "window_size = 2  # Window size in seconds\n",
    "overlap = 0.5\n",
    "\n",
    "# Function to extract label from filename\n",
    "def extract_label(filename):\n",
    "    match = re.match(r'([a-zA-Z_]+)', filename)\n",
    "    return match.group(1) if match else filename\n",
    "\n",
    "# Process all CSV files in the directory and subdirectories\n",
    "all_data = []\n",
    "for dirpath, _, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            label = extract_label(filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            # cleaned_data = data\n",
    "            cleaned_data = cleaning101(data)\n",
    "            processed_data = segment_and_flatten_magnitude(\n",
    "                label=label, magnitude_column_name=magnitude_column_name, \n",
    "                df=cleaned_data, window_size_sec=window_size, overlap=overlap\n",
    "            )\n",
    "            all_data.append(processed_data)\n",
    "\n",
    "# Combine all processed data\n",
    "if all_data:\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"Merged DataFrame shape:\", merged_df.shape)\n",
    "else:\n",
    "    print(\"No CSV files found.\")\n",
    "\n",
    "\n",
    "# compute flatten data with fft\n",
    "fft_df = compute_fft_on_flattened_data(merged_df, num_samples=merged_df.shape[1]-2, remove_flatten=True) # -2 for start_time and label column on df\n",
    "\n",
    "df = fft_df.copy()\n",
    "\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_test_raw = X_test\n",
    "X_train_raw = X_train\n",
    "# 4) Normalize features (0-1 scaling)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 5) Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7) Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i working on digital signal processing project that use for regcognize human activity using accelerometer on android phone \n",
    "# like walking, running, cliambing stair and etc. as you can see my code work well with data that collect and convert into csv file. \n",
    "# Raw csv file have 3 columns is time in millis second x, y, z is raw value of accelerometer. My sampling rate is 50 Hz\n",
    "\n",
    "# this is my old process data flow \n",
    "# Step 1 combine x,y,z with calculate_magnitude function \n",
    "# Step 2 pass calculated magnitude into band pass filter \n",
    "# Step 3 flatten with window size 2 (second) and overlab 50%\n",
    "# Step 4 compute fft with flattened data and keep first half (+1 for DC component)\n",
    "# Step 5 Training model\n",
    "    # encode class name(string) into number with labelencoder\n",
    "    # split train and test\n",
    "    # Normalize features (0-1 scaling) using minmaxscaler\n",
    "    # train random forest model\n",
    "# Step 6 Evaluate model with classification report\n",
    "\n",
    "# Create me an pipeline follow this step\n",
    "# Step 1 read raw csv file this file have these column time, x, y, z, label\n",
    "# Step 2 add column magnitude by compute calculate_magnitude function using columns x, y, z\n",
    "# Step 3 flatten raw magnitude from step 2 with window size 2 second and overlab 50% and add label base on my old segment_and_flatten_magnitude function\n",
    "# Step 4 encode label using labelencoder\n",
    "# Step 5 split train and test (X is raw flattened magnitude and y is label)\n",
    "# Step 6 Create pipeline with\n",
    "    # pipeline 1: apply band pass filter into magnitude\n",
    "    # pipeline 2: compute fft with flattened data and keep first half (+1 for DC component)\n",
    "    # pipeline 3: train random forest model\n",
    "\n",
    "# Then export this pipeline and give an example to use this pipeline for predict new raw calculated magnitude\n",
    "\n",
    "# Follow my step you can see my code below. Then create me an pipeline for combine all these step for easy to deploy and save pipeline with joblip\n",
    "# and give me example for use that pipeline. Make that pipeline easy to predict like just feed raw magnitude (not filter) and model can predict final \n",
    "\n",
    "\n",
    "# Here is my python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to calculate magnitude\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "# Function to apply band-pass filter\n",
    "def bandpass_filter(data, lowcut=0.4, highcut=15, fs=50, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "# Function to segment and flatten magnitude\n",
    "def segment_and_flatten(df, window_size_sec=2, overlap=0.5, sampling_rate=50):\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "    window_samples = int(window_size_sec * sampling_rate)\n",
    "    step_size = int(window_samples * (1 - overlap))\n",
    "    flattened_rows = []\n",
    "\n",
    "    for start_idx in range(0, len(df) - window_samples + 1, step_size):\n",
    "        window = df.iloc[start_idx : start_idx + window_samples].reset_index(drop=True)\n",
    "        row_dict = {'start_time': window.loc[0, 'time'], 'label': label}\n",
    "        for i in range(window_samples):\n",
    "            row_dict[f'x{i+1}'] = window.loc[i, 'magnitude']\n",
    "        flattened_rows.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(flattened_rows)\n",
    "\n",
    "# Function to compute FFT\n",
    "def compute_fft(df, num_samples=100):\n",
    "    feature_columns = [f'x{i+1}' for i in range(num_samples)]\n",
    "    fft_features = []\n",
    "    for _, row in df.iterrows():\n",
    "        ts = row[feature_columns].values.astype(np.float64)\n",
    "        fft_vals = np.fft.fft(ts)\n",
    "        fft_mag = np.abs(fft_vals)\n",
    "        fft_half = fft_mag[:num_samples // 2 + 1]\n",
    "        fft_features.append(fft_half)\n",
    "    fft_df = pd.DataFrame(fft_features, columns=[f'fft_{i+1}' for i in range(num_samples // 2 + 1)])\n",
    "    return pd.concat([df[['start_time', 'label']], fft_df], axis=1)\n",
    "\n",
    "# Load and process dataset\n",
    "def load_and_process_dataset(df):\n",
    "    df['magnitude'] = calculate_magnitude(df['x'], df['y'], df['z'])\n",
    "    df['filtered_magnitude'] = bandpass_filter(df['magnitude'])\n",
    "    segmented_data = segment_and_flatten(df)\n",
    "    fft_data = compute_fft(segmented_data, num_samples=segmented_data.shape[1] - 2)\n",
    "    return fft_data\n",
    "\n",
    "# Train model\n",
    "def train_pipeline(df):\n",
    "    df = load_and_process_dataset(df)\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")\n",
    "\n",
    "# Predict function\n",
    "def predict_new_data(raw_magnitude):\n",
    "    data = joblib.load('activity_recognition_pipeline.pkl')\n",
    "    pipeline = data['pipeline']\n",
    "    le = data['label_encoder']\n",
    "    raw_magnitude = np.array(raw_magnitude).reshape(1, -1)\n",
    "    prediction = pipeline.predict(raw_magnitude)\n",
    "    return le.inverse_transform(prediction)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Version 2 from gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from preprocess import PreprocessAccel\n",
    "\n",
    "pre_acc = PreprocessAccel(sampling_rate=50)\n",
    "\n",
    "# Function to calculate magnitude\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "# Function to apply band-pass filter\n",
    "def bandpass_filter(data, lowcut=0.4, highcut=15, fs=50, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "# Function to compute FFT\n",
    "def compute_fft(data):\n",
    "    fft_vals = np.fft.fft(data)\n",
    "    fft_mag = np.abs(fft_vals)\n",
    "    return fft_mag[: len(fft_mag) // 2 + 1]\n",
    "\n",
    "# Load and process dataset\n",
    "def load_and_process_dataset(df):\n",
    "    df['magnitude'] = calculate_magnitude(df['x'], df['y'], df['z'])\n",
    "    df['filtered_magnitude'] = bandpass_filter(df['magnitude'])\n",
    "    return df\n",
    "\n",
    "# Train model\n",
    "def train_pipeline(df, hidden_layer_sizes=(80, 50, 30, 20, 10), max_iter=500):\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                          activation='relu', solver='adam', \n",
    "                          max_iter=max_iter, random_state=42, verbose=True)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")\n",
    "\n",
    "# Predict function\n",
    "def predict_new_data(raw_magnitude):\n",
    "    data = joblib.load('activity_recognition_pipeline.pkl')\n",
    "    pipeline = data['pipeline']\n",
    "    le = data['label_encoder']\n",
    "    filtered_magnitude = bandpass_filter(raw_magnitude)\n",
    "    fft_features = compute_fft(filtered_magnitude)\n",
    "    fft_features = np.array(fft_features).reshape(1, -1)\n",
    "    prediction = pipeline.predict(fft_features)\n",
    "    return le.inverse_transform(prediction)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82885305\n",
      "Iteration 2, loss = 1.74479124\n",
      "Iteration 3, loss = 1.63234133\n",
      "Iteration 4, loss = 1.53174251\n",
      "Iteration 5, loss = 1.47677147\n",
      "Iteration 6, loss = 1.43360278\n",
      "Iteration 7, loss = 1.38600729\n",
      "Iteration 8, loss = 1.33808302\n",
      "Iteration 9, loss = 1.28363281\n",
      "Iteration 10, loss = 1.22048819\n",
      "Iteration 11, loss = 1.14113649\n",
      "Iteration 12, loss = 1.06295142\n",
      "Iteration 13, loss = 0.98912755\n",
      "Iteration 14, loss = 0.91619080\n",
      "Iteration 15, loss = 0.84324446\n",
      "Iteration 16, loss = 0.77871126\n",
      "Iteration 17, loss = 0.72884838\n",
      "Iteration 18, loss = 0.68184137\n",
      "Iteration 19, loss = 0.64990667\n",
      "Iteration 20, loss = 0.59781218\n",
      "Iteration 21, loss = 0.56032557\n",
      "Iteration 22, loss = 0.52623665\n",
      "Iteration 23, loss = 0.49841936\n",
      "Iteration 24, loss = 0.46969310\n",
      "Iteration 25, loss = 0.43640683\n",
      "Iteration 26, loss = 0.41063183\n",
      "Iteration 27, loss = 0.38390890\n",
      "Iteration 28, loss = 0.36053940\n",
      "Iteration 29, loss = 0.33842805\n",
      "Iteration 30, loss = 0.30568110\n",
      "Iteration 31, loss = 0.28024239\n",
      "Iteration 32, loss = 0.25930062\n",
      "Iteration 33, loss = 0.24029836\n",
      "Iteration 34, loss = 0.22816979\n",
      "Iteration 35, loss = 0.21596613\n",
      "Iteration 36, loss = 0.20679987\n",
      "Iteration 37, loss = 0.19948756\n",
      "Iteration 38, loss = 0.18845056\n",
      "Iteration 39, loss = 0.19333775\n",
      "Iteration 40, loss = 0.18373427\n",
      "Iteration 41, loss = 0.17071115\n",
      "Iteration 42, loss = 0.18436669\n",
      "Iteration 43, loss = 0.16542107\n",
      "Iteration 44, loss = 0.16115531\n",
      "Iteration 45, loss = 0.15145552\n",
      "Iteration 46, loss = 0.15008195\n",
      "Iteration 47, loss = 0.15667173\n",
      "Iteration 48, loss = 0.15485544\n",
      "Iteration 49, loss = 0.14783297\n",
      "Iteration 50, loss = 0.14122960\n",
      "Iteration 51, loss = 0.14230535\n",
      "Iteration 52, loss = 0.13083077\n",
      "Iteration 53, loss = 0.12552378\n",
      "Iteration 54, loss = 0.12536968\n",
      "Iteration 55, loss = 0.12663069\n",
      "Iteration 56, loss = 0.11923330\n",
      "Iteration 57, loss = 0.13747029\n",
      "Iteration 58, loss = 0.11703736\n",
      "Iteration 59, loss = 0.12235700\n",
      "Iteration 60, loss = 0.10968865\n",
      "Iteration 61, loss = 0.10356426\n",
      "Iteration 62, loss = 0.10453314\n",
      "Iteration 63, loss = 0.10203422\n",
      "Iteration 64, loss = 0.09584677\n",
      "Iteration 65, loss = 0.09397772\n",
      "Iteration 66, loss = 0.09379197\n",
      "Iteration 67, loss = 0.09365108\n",
      "Iteration 68, loss = 0.08779857\n",
      "Iteration 69, loss = 0.09454210\n",
      "Iteration 70, loss = 0.09798511\n",
      "Iteration 71, loss = 0.08741176\n",
      "Iteration 72, loss = 0.08255011\n",
      "Iteration 73, loss = 0.08241302\n",
      "Iteration 74, loss = 0.08478022\n",
      "Iteration 75, loss = 0.07983977\n",
      "Iteration 76, loss = 0.07886850\n",
      "Iteration 77, loss = 0.07684041\n",
      "Iteration 78, loss = 0.07580153\n",
      "Iteration 79, loss = 0.07328100\n",
      "Iteration 80, loss = 0.06947962\n",
      "Iteration 81, loss = 0.07133809\n",
      "Iteration 82, loss = 0.06712657\n",
      "Iteration 83, loss = 0.06688202\n",
      "Iteration 84, loss = 0.07233800\n",
      "Iteration 85, loss = 0.06793504\n",
      "Iteration 86, loss = 0.06523837\n",
      "Iteration 87, loss = 0.06600090\n",
      "Iteration 88, loss = 0.07057677\n",
      "Iteration 89, loss = 0.07012598\n",
      "Iteration 90, loss = 0.06311555\n",
      "Iteration 91, loss = 0.07110044\n",
      "Iteration 92, loss = 0.05943274\n",
      "Iteration 93, loss = 0.05672748\n",
      "Iteration 94, loss = 0.05721155\n",
      "Iteration 95, loss = 0.05619726\n",
      "Iteration 96, loss = 0.05457115\n",
      "Iteration 97, loss = 0.05534723\n",
      "Iteration 98, loss = 0.05286109\n",
      "Iteration 99, loss = 0.05103814\n",
      "Iteration 100, loss = 0.05191167\n",
      "Iteration 101, loss = 0.05280769\n",
      "Iteration 102, loss = 0.05456613\n",
      "Iteration 103, loss = 0.05148459\n",
      "Iteration 104, loss = 0.05382657\n",
      "Iteration 105, loss = 0.04876588\n",
      "Iteration 106, loss = 0.04512785\n",
      "Iteration 107, loss = 0.04405988\n",
      "Iteration 108, loss = 0.04307757\n",
      "Iteration 109, loss = 0.04468356\n",
      "Iteration 110, loss = 0.04392928\n",
      "Iteration 111, loss = 0.04522330\n",
      "Iteration 112, loss = 0.04357387\n",
      "Iteration 113, loss = 0.04325277\n",
      "Iteration 114, loss = 0.04438867\n",
      "Iteration 115, loss = 0.04202766\n",
      "Iteration 116, loss = 0.03976502\n",
      "Iteration 117, loss = 0.04097558\n",
      "Iteration 118, loss = 0.04398009\n",
      "Iteration 119, loss = 0.04142093\n",
      "Iteration 120, loss = 0.05263811\n",
      "Iteration 121, loss = 0.04390053\n",
      "Iteration 122, loss = 0.03912760\n",
      "Iteration 123, loss = 0.03626520\n",
      "Iteration 124, loss = 0.03489324\n",
      "Iteration 125, loss = 0.03619513\n",
      "Iteration 126, loss = 0.03483453\n",
      "Iteration 127, loss = 0.03587204\n",
      "Iteration 128, loss = 0.03439063\n",
      "Iteration 129, loss = 0.03190456\n",
      "Iteration 130, loss = 0.03116604\n",
      "Iteration 131, loss = 0.03008539\n",
      "Iteration 132, loss = 0.03063479\n",
      "Iteration 133, loss = 0.02997807\n",
      "Iteration 134, loss = 0.02945761\n",
      "Iteration 135, loss = 0.02987690\n",
      "Iteration 136, loss = 0.03011812\n",
      "Iteration 137, loss = 0.03081569\n",
      "Iteration 138, loss = 0.02963678\n",
      "Iteration 139, loss = 0.02759170\n",
      "Iteration 140, loss = 0.02892152\n",
      "Iteration 141, loss = 0.02653741\n",
      "Iteration 142, loss = 0.02865931\n",
      "Iteration 143, loss = 0.03418785\n",
      "Iteration 144, loss = 0.03208379\n",
      "Iteration 145, loss = 0.03117824\n",
      "Iteration 146, loss = 0.02740896\n",
      "Iteration 147, loss = 0.02920218\n",
      "Iteration 148, loss = 0.02745755\n",
      "Iteration 149, loss = 0.02674392\n",
      "Iteration 150, loss = 0.02692036\n",
      "Iteration 151, loss = 0.02775009\n",
      "Iteration 152, loss = 0.02571446\n",
      "Iteration 153, loss = 0.02415278\n",
      "Iteration 154, loss = 0.02311349\n",
      "Iteration 155, loss = 0.02190264\n",
      "Iteration 156, loss = 0.02246655\n",
      "Iteration 157, loss = 0.02069972\n",
      "Iteration 158, loss = 0.02047215\n",
      "Iteration 159, loss = 0.02183149\n",
      "Iteration 160, loss = 0.02348484\n",
      "Iteration 161, loss = 0.02098732\n",
      "Iteration 162, loss = 0.01965613\n",
      "Iteration 163, loss = 0.02101019\n",
      "Iteration 164, loss = 0.01945859\n",
      "Iteration 165, loss = 0.01855248\n",
      "Iteration 166, loss = 0.01737282\n",
      "Iteration 167, loss = 0.02067978\n",
      "Iteration 168, loss = 0.02082981\n",
      "Iteration 169, loss = 0.02133433\n",
      "Iteration 170, loss = 0.02227003\n",
      "Iteration 171, loss = 0.01780988\n",
      "Iteration 172, loss = 0.01716105\n",
      "Iteration 173, loss = 0.01754601\n",
      "Iteration 174, loss = 0.01688085\n",
      "Iteration 175, loss = 0.01578681\n",
      "Iteration 176, loss = 0.01526213\n",
      "Iteration 177, loss = 0.01524397\n",
      "Iteration 178, loss = 0.01459556\n",
      "Iteration 179, loss = 0.01416693\n",
      "Iteration 180, loss = 0.01434999\n",
      "Iteration 181, loss = 0.01336333\n",
      "Iteration 182, loss = 0.01353930\n",
      "Iteration 183, loss = 0.01295370\n",
      "Iteration 184, loss = 0.01307429\n",
      "Iteration 185, loss = 0.01278237\n",
      "Iteration 186, loss = 0.01282065\n",
      "Iteration 187, loss = 0.01222833\n",
      "Iteration 188, loss = 0.01242953\n",
      "Iteration 189, loss = 0.01180436\n",
      "Iteration 190, loss = 0.01155531\n",
      "Iteration 191, loss = 0.01206280\n",
      "Iteration 192, loss = 0.01230786\n",
      "Iteration 193, loss = 0.01145163\n",
      "Iteration 194, loss = 0.01130725\n",
      "Iteration 195, loss = 0.01091090\n",
      "Iteration 196, loss = 0.01102685\n",
      "Iteration 197, loss = 0.01051859\n",
      "Iteration 198, loss = 0.01048562\n",
      "Iteration 199, loss = 0.01061611\n",
      "Iteration 200, loss = 0.01086882\n",
      "Iteration 201, loss = 0.01071436\n",
      "Iteration 202, loss = 0.01078398\n",
      "Iteration 203, loss = 0.00944533\n",
      "Iteration 204, loss = 0.01022336\n",
      "Iteration 205, loss = 0.00953737\n",
      "Iteration 206, loss = 0.00944561\n",
      "Iteration 207, loss = 0.00979868\n",
      "Iteration 208, loss = 0.00818029\n",
      "Iteration 209, loss = 0.00895507\n",
      "Iteration 210, loss = 0.00835430\n",
      "Iteration 211, loss = 0.00811817\n",
      "Iteration 212, loss = 0.00829870\n",
      "Iteration 213, loss = 0.00777794\n",
      "Iteration 214, loss = 0.00762454\n",
      "Iteration 215, loss = 0.00764681\n",
      "Iteration 216, loss = 0.00808189\n",
      "Iteration 217, loss = 0.00783706\n",
      "Iteration 218, loss = 0.00713565\n",
      "Iteration 219, loss = 0.00682256\n",
      "Iteration 220, loss = 0.00677489\n",
      "Iteration 221, loss = 0.00665444\n",
      "Iteration 222, loss = 0.00670101\n",
      "Iteration 223, loss = 0.00626347\n",
      "Iteration 224, loss = 0.00664702\n",
      "Iteration 225, loss = 0.00673327\n",
      "Iteration 226, loss = 0.00791175\n",
      "Iteration 227, loss = 0.00683665\n",
      "Iteration 228, loss = 0.00645650\n",
      "Iteration 229, loss = 0.00623401\n",
      "Iteration 230, loss = 0.00618942\n",
      "Iteration 231, loss = 0.00563929\n",
      "Iteration 232, loss = 0.00555459\n",
      "Iteration 233, loss = 0.00568215\n",
      "Iteration 234, loss = 0.00516886\n",
      "Iteration 235, loss = 0.00504507\n",
      "Iteration 236, loss = 0.00476107\n",
      "Iteration 237, loss = 0.00465375\n",
      "Iteration 238, loss = 0.00473995\n",
      "Iteration 239, loss = 0.00532833\n",
      "Iteration 240, loss = 0.00484002\n",
      "Iteration 241, loss = 0.00475492\n",
      "Iteration 242, loss = 0.00424898\n",
      "Iteration 243, loss = 0.00413630\n",
      "Iteration 244, loss = 0.00415983\n",
      "Iteration 245, loss = 0.00419592\n",
      "Iteration 246, loss = 0.00398122\n",
      "Iteration 247, loss = 0.00404625\n",
      "Iteration 248, loss = 0.00408986\n",
      "Iteration 249, loss = 0.00381623\n",
      "Iteration 250, loss = 0.00376546\n",
      "Iteration 251, loss = 0.00440203\n",
      "Iteration 252, loss = 0.00400421\n",
      "Iteration 253, loss = 0.00367578\n",
      "Iteration 254, loss = 0.00340310\n",
      "Iteration 255, loss = 0.00339119\n",
      "Iteration 256, loss = 0.00373781\n",
      "Iteration 257, loss = 0.00345158\n",
      "Iteration 258, loss = 0.00324538\n",
      "Iteration 259, loss = 0.00308481\n",
      "Iteration 260, loss = 0.00307294\n",
      "Iteration 261, loss = 0.00314349\n",
      "Iteration 262, loss = 0.00309547\n",
      "Iteration 263, loss = 0.00297211\n",
      "Iteration 264, loss = 0.00282716\n",
      "Iteration 265, loss = 0.00281790\n",
      "Iteration 266, loss = 0.00280800\n",
      "Iteration 267, loss = 0.00266908\n",
      "Iteration 268, loss = 0.00271073\n",
      "Iteration 269, loss = 0.00266182\n",
      "Iteration 270, loss = 0.00272425\n",
      "Iteration 271, loss = 0.00277456\n",
      "Iteration 272, loss = 0.00265953\n",
      "Iteration 273, loss = 0.00244938\n",
      "Iteration 274, loss = 0.00242543\n",
      "Iteration 275, loss = 0.00234404\n",
      "Iteration 276, loss = 0.00233093\n",
      "Iteration 277, loss = 0.00277134\n",
      "Iteration 278, loss = 0.00219784\n",
      "Iteration 279, loss = 0.00286712\n",
      "Iteration 280, loss = 0.00238300\n",
      "Iteration 281, loss = 0.00226212\n",
      "Iteration 282, loss = 0.00216779\n",
      "Iteration 283, loss = 0.00211411\n",
      "Iteration 284, loss = 0.00204541\n",
      "Iteration 285, loss = 0.00198461\n",
      "Iteration 286, loss = 0.00206722\n",
      "Iteration 287, loss = 0.00202415\n",
      "Iteration 288, loss = 0.00199796\n",
      "Iteration 289, loss = 0.00192203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.89      0.96      0.93        53\n",
      "          descending_stairs       0.91      0.92      0.91        73\n",
      "                    nothing       1.00      1.00      1.00        54\n",
      "                    running       1.00      0.98      0.99        53\n",
      "sitting_standing_transition       0.98      0.98      0.98        64\n",
      "                    walking       0.95      0.90      0.92        67\n",
      "\n",
      "                   accuracy                           0.95       364\n",
      "                  macro avg       0.96      0.96      0.96       364\n",
      "               weighted avg       0.95      0.95      0.95       364\n",
      "\n",
      "Pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = r'D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win5_lab50.csv'\n",
    "train_pipeline(pd.read_csv(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted activity: climbing_stairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\final_project_dsp\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_magnitude = [-1.9269720474086889,-3.9535963264726286,-3.610971491888949,-1.5433760008604405,-1.0043608153004953,-1.9522487696309168,-1.999174826152136,-1.043848946817768,-0.4439468039369165,-0.4566944243320182,-0.5926976645717903,-0.5908331289349239,-0.4056274775312546,-0.0694776144812227,0.2828222622289001,0.5267270334145863,0.6982578625378797,0.9474634251276348,1.2885017430589882,1.4858023337644424,1.430525202945011,1.4563666249239935,1.8158920460947787,2.2000462505921936,2.12756586082568,1.4975846707645968,0.972249513894111,1.301180820290731,2.0123606308095234,2.162261137596188,1.5793890019468768,0.3827977373727375,-1.2119609816043235,-2.459902067055804,-2.666356906532143,-2.481980196564871,-3.018674754067028,-3.519102542265938,-2.6536894542926195,-1.0332864090202758,-0.070148167838492,-0.1790101227123757,-0.7484022604247628,-0.9467005668292648,-0.8014474142761727,-0.9281826034890698,-1.2366379257751825,-1.0753412035207053,-0.3322344225804168,0.3628425988153955,0.5393957407884269,0.5830972494506156,0.9613680149845492,1.3357289531582752,1.3514832210972518,1.4206131162721622,1.68262701986672,1.5580493159841502,1.2508935629139624,1.660939949364311,2.565976800171682,2.9096783168570948,2.2424480890876985,0.8658109422943936,-0.7987488461969393,-2.486874517578699,-3.5519096491785698,-3.3399223152158948,-2.318179969057168,-1.4094299334456757,-0.9352914011430322,-0.8219382569032276,-0.8991242537729679,-0.9398732494449176,-0.8310274050188793,-0.7228645106825701,-0.7510581406447109,-0.7381344693296573,-0.4930819479945015,-0.1542708295096546,0.0284438009471157,0.1220863638406615,0.3657008699281848,0.6690254623208504,0.9497325183856876,1.4821707621709024,2.141504495114866,2.474538318291468,2.3603135965983424,1.6444185724062756,0.7777831799208994,0.9755491479323568,1.975839468590048,2.4482037062147004,2.3999483239327843,2.0909369476494297,0.8924821288379312,-0.978097045438806,-2.29987317233632,-2.5981731046374943,-2.210980446452718,-1.5691300326582116,-1.3473083049818868,-1.95944097144001,-2.6775893447021084,-2.6945580052223823,-2.401199596941323,-2.2588896994004517,-2.0337073114692195,-1.646284320723402,-1.1032705519467392,-0.3587025724412591,0.3244097461288157,0.7678285201254511,0.9848272918582684,1.0794026186193304,1.3327719237696032,1.6817339757651468,1.877161536782004,2.1249046552208792,2.461519670047495,2.533813341497132,2.314977790147115,2.0017182939030835,1.8916567860968063,2.2341138850389104,2.565135576455762,2.293103710547705,1.4933040255747392,0.1923453775197899,-1.640425977970132,-3.205761189714095,-3.610038644520169,-2.9646300917863497,-2.0731264280508803,-1.650244066315334,-1.6610305559776517,-1.618938444994348,-1.469779227659456,-1.4419272549603843,-1.5711910928288049,-1.6795934280613485,-1.5603054033236847,-1.2277690209581662,-0.7993304404616814,-0.4029360048775012,-0.1634029563760262,0.1693751551831318,0.8427253647422082,1.4942101728972284,2.0229793904947444,2.9424304289826506,4.000556960810066,4.204956740724863,3.4243847436159247,2.3268418022257262,1.3989586795819091,0.9966145197092076,1.291708419625052,1.700899119757892,1.2830036070716635,0.1230212562593542,-0.9581157808738018,-1.9596140727414189,-2.944568692854584,-3.2671521863682083,-3.03437501921387,-2.889143474906973,-2.699603575813858,-2.3046555019983805,-1.9733610462578863,-1.701956256748684,-1.41261635328417,-1.194631095024203,-0.9610527270931296,-0.7138555537372812,-0.6456777413138093,-0.6505699239485324,-0.4943131880800978,-0.109508566695326,0.5848939502577961,1.2976389604814025,1.3380442678598516,0.9402296394941344,1.12257844334081,1.8098644195539917,2.109203942485512,1.9864754172332453,1.977111793048882,2.2240786652146065,2.6871984421202653,3.176933545296387,3.088378073188342,2.246547249667172,1.379274339782507,0.6521979988197503,-0.7563041784186838,-2.6900940609522843,-3.650825592494744,-2.909330093941332,-1.65271075805177,-1.4811717152950346,-2.055958273304232,-1.8728267415950144,-0.97254591691293,-0.524532178761456,-0.776968942682341,-1.0306447280422102,-0.7836006827310475,-0.3900892912000824,-0.4425135542328332,-0.6044816509149412,-0.2188095043825917,0.3205474840284611,0.4355232604877098,0.5108945148963782,0.9613020527912634,1.528395904288081,1.8291086350931507,1.871443535082636,1.9419194074282489,1.924634176566096,1.4926010207270948,0.9509438606580184,0.7442478227815879,0.8658160358410039,1.223625553659427,1.7525586361993717,2.2609100170923675,1.9214408622428472,-0.0474500142030707,-2.216141662567205,-2.623113859181798,-2.145548034692475,-2.322759725969924,-2.677601733998462,-2.614068875721906,-2.363115665113501,-1.8493477371264655,-1.2320038606783066,-1.1707388533367595,-1.453275804407009,-1.3109130437147578,-0.8668969470535376,-0.613003649769417,-0.432636088535272,-0.0978053981660688,0.3139758029769334,0.7174644270761752,1.0379724676543065]\n",
    "print(\"Predicted activity:\", predict_new_data(example_magnitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (300420, 5)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      D:\\git       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n",
      "Pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\cleaned_data_many_position\"\n",
    "\n",
    "# Function to extract label from filename\n",
    "def extract_label(filename):\n",
    "    match = re.match(r'([a-zA-Z]+)', filename)\n",
    "    return match.group(1) if match else filename\n",
    "\n",
    "# Process all CSV files in the directory and subdirectories\n",
    "all_data = []\n",
    "for dirpath, _, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            label = extract_label(filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            # print(data.sample())\n",
    "            data['label'] = label\n",
    "            all_data.append(data.drop(['filtered_magnitude', 'magnitude'], axis=1))\n",
    "\n",
    "# Combine all processed data\n",
    "if all_data:\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"Merged DataFrame shape:\", merged_df.shape)\n",
    "else:\n",
    "    print(\"No CSV files found.\")\n",
    "\n",
    "train_pipeline(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24181</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>9.04</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24208</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>9.60</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24235</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>10.29</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24261</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>10.24</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24283</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>9.80</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300415</th>\n",
       "      <td>269668</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300416</th>\n",
       "      <td>269689</td>\n",
       "      <td>-2.57</td>\n",
       "      <td>8.96</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300417</th>\n",
       "      <td>269712</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>9.20</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300418</th>\n",
       "      <td>269735</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>9.42</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300419</th>\n",
       "      <td>269756</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>9.24</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300420 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time     x      y     z     label\n",
       "0        24181 -1.86   9.04 -1.62  climbing\n",
       "1        24208 -1.33   9.60 -2.19  climbing\n",
       "2        24235 -1.32  10.29 -2.32  climbing\n",
       "3        24261 -1.81  10.24 -2.96  climbing\n",
       "4        24283 -1.85   9.80 -3.04  climbing\n",
       "...        ...   ...    ...   ...       ...\n",
       "300415  269668 -2.74   8.79 -0.99   walking\n",
       "300416  269689 -2.57   8.96 -1.08   walking\n",
       "300417  269712 -2.03   9.20 -1.05   walking\n",
       "300418  269735 -2.54   9.42 -1.11   walking\n",
       "300419  269756 -2.83   9.24 -1.23   walking\n",
       "\n",
       "[300420 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
