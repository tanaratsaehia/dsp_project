{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import os\n",
    "\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def cleaning101(data):\n",
    "    df = data.copy()\n",
    "    x = np.array(df['x'].values)\n",
    "    y = np.array(df['y'].values)\n",
    "    z = np.array(df['z'].values)\n",
    "    magnitude = calculate_magnitude(x, y, z)\n",
    "    df['magnitude'] = magnitude\n",
    "\n",
    "    fs = 50  # Sampling frequency\n",
    "    lowcut = 0.4  # Lower cutoff frequency (Hz)\n",
    "    highcut = 15  # Upper cutoff frequency (Hz)\n",
    "    signal = np.array(df['magnitude'])\n",
    "\n",
    "    filtered_data = bandpass_filter(signal, lowcut, highcut, fs)\n",
    "    df['filtered_magnitude'] = filtered_data\n",
    "    # df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "def segment_and_flatten_magnitude(label, df, magnitude_column_name='filtered_magnitude', window_size_sec=5, overlap=0.5, sampling_rate=50):\n",
    "    \"\"\"\n",
    "    1) Combine x, y, z signals into a single magnitude signal.\n",
    "    2) Segment the magnitude signal into windows of window_size_sec seconds \n",
    "    with the specified overlap.\n",
    "    3) Flatten each window into columns: start_time, x1, x2, ..., xN \n",
    "    (where N = window_size_sec * sampling_rate).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing columns 'time', 'x', 'y', 'z'.\n",
    "                        'time' is in milliseconds.\n",
    "        magnitude_column_name (string): Target column name that already calculate magnitude.\n",
    "        window_size_sec (float): Length of each window in seconds (default 5).\n",
    "        overlap (float): Fraction of window overlap (default 0.5 = 50%).\n",
    "        sampling_rate (int): Sampling rate in Hz (default 50).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Each row corresponds to a flattened window.\n",
    "                    Columns:\n",
    "                    - 'start_time': millisecond timestamp of the window start\n",
    "                    - 'x1', 'x2', ..., 'xN': magnitude values for each sample\n",
    "    \"\"\"\n",
    "    # Sort by time to ensure chronological order\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "    # 2) Calculate number of samples per window and step size\n",
    "    window_samples = int(window_size_sec * sampling_rate)  # e.g., 5s * 50Hz = 250\n",
    "    step_size = int(window_samples * (1 - overlap))        # e.g., 250 * 0.5 = 125\n",
    "\n",
    "    flattened_rows = []\n",
    "\n",
    "    # 3) Loop through data with the given step size\n",
    "    for start_idx in range(0, len(df) - window_samples + 1, step_size):\n",
    "        # Extract this window of magnitude values\n",
    "        window = df.iloc[start_idx : start_idx + window_samples].reset_index(drop=True)\n",
    "        \n",
    "        # Prepare a dict for one flattened row\n",
    "        row_dict = {}\n",
    "        row_dict['start_time'] = window.loc[0, 'time']\n",
    "        row_dict['label'] = label\n",
    "        \n",
    "        # Flatten magnitude into x1, x2, x3, ..., xN\n",
    "        for i in range(window_samples):\n",
    "            row_dict[f'x{i+1}'] = window.loc[i, magnitude_column_name]\n",
    "        \n",
    "        flattened_rows.append(row_dict)\n",
    "\n",
    "    # Convert list of dicts into a DataFrame\n",
    "    return pd.DataFrame(flattened_rows)\n",
    "\n",
    "def compute_fft_on_flattened_data(df, num_samples=250, remove_flatten=True):\n",
    "    \"\"\"\n",
    "    Compute the FFT of the time-series in columns x1...xN, take the magnitude spectrum, \n",
    "    and keep only the first half (non-redundant part).\n",
    "    \n",
    "    The returned DataFrame contains:\n",
    "    - 'start_time' and 'label'\n",
    "    - Original data (x1-x250) if remove_flatten=False\n",
    "    - FFT features starting from x251 (fft1-fft126)\n",
    "    \"\"\"\n",
    "    feature_columns = [f'x{i+1}' for i in range(num_samples)]\n",
    "    fft_features_list = []\n",
    "\n",
    "    # Loop through each row to compute FFT\n",
    "    for _, row in df.iterrows():\n",
    "        ts = row[feature_columns].values.astype(np.float64)\n",
    "        fft_vals = np.fft.fft(ts)\n",
    "        fft_mag = np.abs(fft_vals)\n",
    "        half_fft = fft_mag[:num_samples // 2 + 1]  # Keep first half (+1 for DC component)\n",
    "        fft_features_list.append(half_fft)\n",
    "\n",
    "    # Create FFT feature DataFrame\n",
    "    num_fft_features = num_samples // 2 + 1\n",
    "    fft_df = pd.DataFrame(\n",
    "        fft_features_list, \n",
    "        columns=[f'x{i+1}' for i in range(num_samples+1, num_samples+1+num_fft_features)]\n",
    "    )\n",
    "\n",
    "    # Include original data if required\n",
    "    if not remove_flatten:\n",
    "        combined_df = pd.concat([df[['start_time', 'label'] + feature_columns].reset_index(drop=True), fft_df], axis=1)\n",
    "    else:\n",
    "        combined_df = pd.concat([df[['start_time', 'label']].reset_index(drop=True), fft_df], axis=1)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (5966, 102)\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.84      0.88      0.86       161\n",
      "          descending_stairs       0.93      0.75      0.83       149\n",
      "                    nothing       0.99      0.93      0.96       253\n",
      "                    running       1.00      1.00      1.00       213\n",
      "sitting_standing_transition       0.91      0.98      0.94       208\n",
      "                    walking       0.88      0.96      0.92       210\n",
      "\n",
      "                   accuracy                           0.93      1194\n",
      "                  macro avg       0.92      0.92      0.92      1194\n",
      "               weighted avg       0.93      0.93      0.93      1194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1 clean and flatten data\n",
    "root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\cleaned_data_many_position\"\n",
    "# root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\clean_datasets\"\n",
    "magnitude_column_name = \"filtered_magnitude\"  # Adjust if needed\n",
    "# magnitude_column_name = \"magnitude\"  # raw magnitude\n",
    "window_size = 2  # Window size in seconds\n",
    "overlap = 0.5\n",
    "\n",
    "# Function to extract label from filename\n",
    "def extract_label(filename):\n",
    "    match = re.match(r'([a-zA-Z_]+)', filename)\n",
    "    return match.group(1) if match else filename\n",
    "\n",
    "# Process all CSV files in the directory and subdirectories\n",
    "all_data = []\n",
    "for dirpath, _, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            label = extract_label(filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            # cleaned_data = data\n",
    "            cleaned_data = cleaning101(data)\n",
    "            processed_data = segment_and_flatten_magnitude(\n",
    "                label=label, magnitude_column_name=magnitude_column_name, \n",
    "                df=cleaned_data, window_size_sec=window_size, overlap=overlap\n",
    "            )\n",
    "            all_data.append(processed_data)\n",
    "\n",
    "# Combine all processed data\n",
    "if all_data:\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"Merged DataFrame shape:\", merged_df.shape)\n",
    "else:\n",
    "    print(\"No CSV files found.\")\n",
    "\n",
    "\n",
    "# compute flatten data with fft\n",
    "fft_df = compute_fft_on_flattened_data(merged_df, num_samples=merged_df.shape[1]-2, remove_flatten=True) # -2 for start_time and label column on df\n",
    "\n",
    "df = fft_df.copy()\n",
    "\n",
    "# 1) Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# 2) Separate features and target\n",
    "X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "y = df['label_encoded']\n",
    "\n",
    "# 3) Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_test_raw = X_test\n",
    "X_train_raw = X_train\n",
    "# 4) Normalize features (0-1 scaling)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 5) Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7) Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i working on digital signal processing project that use for regcognize human activity using accelerometer on android phone \n",
    "# like walking, running, cliambing stair and etc. as you can see my code work well with data that collect and convert into csv file. \n",
    "# Raw csv file have 3 columns is time in millis second x, y, z is raw value of accelerometer. My sampling rate is 50 Hz\n",
    "\n",
    "# this is my old process data flow \n",
    "# Step 1 combine x,y,z with calculate_magnitude function \n",
    "# Step 2 pass calculated magnitude into band pass filter \n",
    "# Step 3 flatten with window size 2 (second) and overlab 50%\n",
    "# Step 4 compute fft with flattened data and keep first half (+1 for DC component)\n",
    "# Step 5 Training model\n",
    "    # encode class name(string) into number with labelencoder\n",
    "    # split train and test\n",
    "    # Normalize features (0-1 scaling) using minmaxscaler\n",
    "    # train random forest model\n",
    "# Step 6 Evaluate model with classification report\n",
    "\n",
    "# Create me an pipeline follow this step\n",
    "# Step 1 read raw csv file this file have these column time, x, y, z, label\n",
    "# Step 2 add column magnitude by compute calculate_magnitude function using columns x, y, z\n",
    "# Step 3 flatten raw magnitude from step 2 with window size 2 second and overlab 50% and add label base on my old segment_and_flatten_magnitude function\n",
    "# Step 4 encode label using labelencoder\n",
    "# Step 5 split train and test (X is raw flattened magnitude and y is label)\n",
    "# Step 6 Create pipeline with\n",
    "    # pipeline 1: apply band pass filter into magnitude\n",
    "    # pipeline 2: compute fft with flattened data and keep first half (+1 for DC component)\n",
    "    # pipeline 3: train random forest model\n",
    "\n",
    "# Then export this pipeline and give an example to use this pipeline for predict new raw calculated magnitude\n",
    "\n",
    "# Follow my step you can see my code below. Then create me an pipeline for combine all these step for easy to deploy and save pipeline with joblip\n",
    "# and give me example for use that pipeline. Make that pipeline easy to predict like just feed raw magnitude (not filter) and model can predict final \n",
    "\n",
    "\n",
    "# Here is my python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to calculate magnitude\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "# Function to apply band-pass filter\n",
    "def bandpass_filter(data, lowcut=0.4, highcut=15, fs=50, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "# Function to segment and flatten magnitude\n",
    "def segment_and_flatten(df, window_size_sec=2, overlap=0.5, sampling_rate=50):\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "    window_samples = int(window_size_sec * sampling_rate)\n",
    "    step_size = int(window_samples * (1 - overlap))\n",
    "    flattened_rows = []\n",
    "\n",
    "    for start_idx in range(0, len(df) - window_samples + 1, step_size):\n",
    "        window = df.iloc[start_idx : start_idx + window_samples].reset_index(drop=True)\n",
    "        row_dict = {'start_time': window.loc[0, 'time'], 'label': label}\n",
    "        for i in range(window_samples):\n",
    "            row_dict[f'x{i+1}'] = window.loc[i, 'magnitude']\n",
    "        flattened_rows.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(flattened_rows)\n",
    "\n",
    "# Function to compute FFT\n",
    "def compute_fft(df, num_samples=100):\n",
    "    feature_columns = [f'x{i+1}' for i in range(num_samples)]\n",
    "    fft_features = []\n",
    "    for _, row in df.iterrows():\n",
    "        ts = row[feature_columns].values.astype(np.float64)\n",
    "        fft_vals = np.fft.fft(ts)\n",
    "        fft_mag = np.abs(fft_vals)\n",
    "        fft_half = fft_mag[:num_samples // 2 + 1]\n",
    "        fft_features.append(fft_half)\n",
    "    fft_df = pd.DataFrame(fft_features, columns=[f'fft_{i+1}' for i in range(num_samples // 2 + 1)])\n",
    "    return pd.concat([df[['start_time', 'label']], fft_df], axis=1)\n",
    "\n",
    "# Load and process dataset\n",
    "def load_and_process_dataset(df):\n",
    "    df['magnitude'] = calculate_magnitude(df['x'], df['y'], df['z'])\n",
    "    df['filtered_magnitude'] = bandpass_filter(df['magnitude'])\n",
    "    segmented_data = segment_and_flatten(df)\n",
    "    fft_data = compute_fft(segmented_data, num_samples=segmented_data.shape[1] - 2)\n",
    "    return fft_data\n",
    "\n",
    "# Train model\n",
    "def train_pipeline(df):\n",
    "    df = load_and_process_dataset(df)\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")\n",
    "\n",
    "# Predict function\n",
    "def predict_new_data(raw_magnitude):\n",
    "    data = joblib.load('activity_recognition_pipeline.pkl')\n",
    "    pipeline = data['pipeline']\n",
    "    le = data['label_encoder']\n",
    "    raw_magnitude = np.array(raw_magnitude).reshape(1, -1)\n",
    "    prediction = pipeline.predict(raw_magnitude)\n",
    "    return le.inverse_transform(prediction)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Version 2 from gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from preprocess import PreprocessAccel\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pre_acc = PreprocessAccel(sampling_rate=50)\n",
    "\n",
    "# Function to calculate magnitude\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "# Function to apply band-pass filter\n",
    "def bandpass_filter(data, lowcut=0.4, highcut=15, fs=50, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "# Function to compute FFT\n",
    "def compute_fft(data):\n",
    "    fft_vals = np.fft.fft(data)\n",
    "    fft_mag = np.abs(fft_vals)\n",
    "    return fft_mag[: len(fft_mag) // 2 + 1]\n",
    "\n",
    "# Load and process dataset\n",
    "def load_and_process_dataset(df):\n",
    "    df['magnitude'] = calculate_magnitude(df['x'], df['y'], df['z'])\n",
    "    df['filtered_magnitude'] = bandpass_filter(df['magnitude'])\n",
    "    return df\n",
    "\n",
    "# Predict function\n",
    "def predict_new_data(raw_magnitude):\n",
    "    data = joblib.load('activity_recognition_pipeline.pkl')\n",
    "    pipeline = data['pipeline']\n",
    "    le = data['label_encoder']\n",
    "    filtered_magnitude = bandpass_filter(raw_magnitude)\n",
    "    fft_features = compute_fft(filtered_magnitude)\n",
    "    fft_features = np.array(fft_features).reshape(1, -1)\n",
    "    prediction = pipeline.predict(fft_features)\n",
    "    return le.inverse_transform(prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_pipeline(df):\n",
    "    # df = load_and_process_dataset(df)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_pipeline(df, max_iter=500):\n",
    "    hidden_layer_sizes=(100, 80, 50, 30, 20, 10) # for data that have filter\n",
    "    # hidden_layer_sizes=(100, 90, 80, 70, 50, 30, 20, 10) # for data that NOT pass filter\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                          activation='relu', solver='adam', \n",
    "                          max_iter=max_iter, random_state=42, verbose=True)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_pipeline(df, n_neighbors=5):\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    X = df.drop(['start_time', 'label', 'label_encoded'], axis=1)\n",
    "    y = df['label_encoded']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    joblib.dump({'pipeline': pipeline, 'label_encoder': le}, 'activity_recognition_pipeline.pkl')\n",
    "    print(\"Pipeline saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "            climbing_stairs       0.79      0.86      0.82       170\n",
      "          descending_stairs       0.91      0.65      0.76       158\n",
      "                    nothing       0.97      0.93      0.95       148\n",
      "                    running       1.00      0.97      0.98       145\n",
      "sitting_standing_transition       0.88      0.99      0.93       146\n",
      "                    walking       0.81      0.94      0.87       154\n",
      "\n",
      "                   accuracy                           0.88       921\n",
      "                  macro avg       0.89      0.89      0.89       921\n",
      "               weighted avg       0.89      0.88      0.88       921\n",
      "\n",
      "Pipeline saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanar\\anaconda3\\envs\\final_project_dsp\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\tanar\\anaconda3\\envs\\final_project_dsp\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\tanar\\anaconda3\\envs\\final_project_dsp\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\tanar\\anaconda3\\envs\\final_project_dsp\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\tanar\\anaconda3\\envs\\final_project_dsp\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win5_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_no_fft_win5_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\combine_time_feq_win5_lab50.csv'\n",
    "\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\NO_FILTER_with_fft_win5_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\NO_FILTER_flattened_no_fft_win5_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\NO_FILTER_combine_time_feq_win5_lab50.csv'\n",
    "\n",
    "file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_with_fft_win2_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\flattened_no_fft_win2_lab50.csv'\n",
    "# file_path = r'C:\\Git-Repository\\dsp_project\\ignore_dir\\datasets\\ready_to_train\\combine_time_feq_win2_lab50.csv'\n",
    "train_pipeline(pd.read_csv(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted activity: climbing_stairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\final_project_dsp\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_magnitude = [-1.9269720474086889,-3.9535963264726286,-3.610971491888949,-1.5433760008604405,-1.0043608153004953,-1.9522487696309168,-1.999174826152136,-1.043848946817768,-0.4439468039369165,-0.4566944243320182,-0.5926976645717903,-0.5908331289349239,-0.4056274775312546,-0.0694776144812227,0.2828222622289001,0.5267270334145863,0.6982578625378797,0.9474634251276348,1.2885017430589882,1.4858023337644424,1.430525202945011,1.4563666249239935,1.8158920460947787,2.2000462505921936,2.12756586082568,1.4975846707645968,0.972249513894111,1.301180820290731,2.0123606308095234,2.162261137596188,1.5793890019468768,0.3827977373727375,-1.2119609816043235,-2.459902067055804,-2.666356906532143,-2.481980196564871,-3.018674754067028,-3.519102542265938,-2.6536894542926195,-1.0332864090202758,-0.070148167838492,-0.1790101227123757,-0.7484022604247628,-0.9467005668292648,-0.8014474142761727,-0.9281826034890698,-1.2366379257751825,-1.0753412035207053,-0.3322344225804168,0.3628425988153955,0.5393957407884269,0.5830972494506156,0.9613680149845492,1.3357289531582752,1.3514832210972518,1.4206131162721622,1.68262701986672,1.5580493159841502,1.2508935629139624,1.660939949364311,2.565976800171682,2.9096783168570948,2.2424480890876985,0.8658109422943936,-0.7987488461969393,-2.486874517578699,-3.5519096491785698,-3.3399223152158948,-2.318179969057168,-1.4094299334456757,-0.9352914011430322,-0.8219382569032276,-0.8991242537729679,-0.9398732494449176,-0.8310274050188793,-0.7228645106825701,-0.7510581406447109,-0.7381344693296573,-0.4930819479945015,-0.1542708295096546,0.0284438009471157,0.1220863638406615,0.3657008699281848,0.6690254623208504,0.9497325183856876,1.4821707621709024,2.141504495114866,2.474538318291468,2.3603135965983424,1.6444185724062756,0.7777831799208994,0.9755491479323568,1.975839468590048,2.4482037062147004,2.3999483239327843,2.0909369476494297,0.8924821288379312,-0.978097045438806,-2.29987317233632,-2.5981731046374943,-2.210980446452718,-1.5691300326582116,-1.3473083049818868,-1.95944097144001,-2.6775893447021084,-2.6945580052223823,-2.401199596941323,-2.2588896994004517,-2.0337073114692195,-1.646284320723402,-1.1032705519467392,-0.3587025724412591,0.3244097461288157,0.7678285201254511,0.9848272918582684,1.0794026186193304,1.3327719237696032,1.6817339757651468,1.877161536782004,2.1249046552208792,2.461519670047495,2.533813341497132,2.314977790147115,2.0017182939030835,1.8916567860968063,2.2341138850389104,2.565135576455762,2.293103710547705,1.4933040255747392,0.1923453775197899,-1.640425977970132,-3.205761189714095,-3.610038644520169,-2.9646300917863497,-2.0731264280508803,-1.650244066315334,-1.6610305559776517,-1.618938444994348,-1.469779227659456,-1.4419272549603843,-1.5711910928288049,-1.6795934280613485,-1.5603054033236847,-1.2277690209581662,-0.7993304404616814,-0.4029360048775012,-0.1634029563760262,0.1693751551831318,0.8427253647422082,1.4942101728972284,2.0229793904947444,2.9424304289826506,4.000556960810066,4.204956740724863,3.4243847436159247,2.3268418022257262,1.3989586795819091,0.9966145197092076,1.291708419625052,1.700899119757892,1.2830036070716635,0.1230212562593542,-0.9581157808738018,-1.9596140727414189,-2.944568692854584,-3.2671521863682083,-3.03437501921387,-2.889143474906973,-2.699603575813858,-2.3046555019983805,-1.9733610462578863,-1.701956256748684,-1.41261635328417,-1.194631095024203,-0.9610527270931296,-0.7138555537372812,-0.6456777413138093,-0.6505699239485324,-0.4943131880800978,-0.109508566695326,0.5848939502577961,1.2976389604814025,1.3380442678598516,0.9402296394941344,1.12257844334081,1.8098644195539917,2.109203942485512,1.9864754172332453,1.977111793048882,2.2240786652146065,2.6871984421202653,3.176933545296387,3.088378073188342,2.246547249667172,1.379274339782507,0.6521979988197503,-0.7563041784186838,-2.6900940609522843,-3.650825592494744,-2.909330093941332,-1.65271075805177,-1.4811717152950346,-2.055958273304232,-1.8728267415950144,-0.97254591691293,-0.524532178761456,-0.776968942682341,-1.0306447280422102,-0.7836006827310475,-0.3900892912000824,-0.4425135542328332,-0.6044816509149412,-0.2188095043825917,0.3205474840284611,0.4355232604877098,0.5108945148963782,0.9613020527912634,1.528395904288081,1.8291086350931507,1.871443535082636,1.9419194074282489,1.924634176566096,1.4926010207270948,0.9509438606580184,0.7442478227815879,0.8658160358410039,1.223625553659427,1.7525586361993717,2.2609100170923675,1.9214408622428472,-0.0474500142030707,-2.216141662567205,-2.623113859181798,-2.145548034692475,-2.322759725969924,-2.677601733998462,-2.614068875721906,-2.363115665113501,-1.8493477371264655,-1.2320038606783066,-1.1707388533367595,-1.453275804407009,-1.3109130437147578,-0.8668969470535376,-0.613003649769417,-0.432636088535272,-0.0978053981660688,0.3139758029769334,0.7174644270761752,1.0379724676543065]\n",
    "print(\"Predicted activity:\", predict_new_data(example_magnitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (300420, 5)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      D:\\git       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n",
      "Pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_path = r\"D:\\git_repository\\dsp_project\\ignore_dir\\datasets\\cleaned_data_many_position\"\n",
    "\n",
    "# Function to extract label from filename\n",
    "def extract_label(filename):\n",
    "    match = re.match(r'([a-zA-Z]+)', filename)\n",
    "    return match.group(1) if match else filename\n",
    "\n",
    "# Process all CSV files in the directory and subdirectories\n",
    "all_data = []\n",
    "for dirpath, _, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            label = extract_label(filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            # print(data.sample())\n",
    "            data['label'] = label\n",
    "            all_data.append(data.drop(['filtered_magnitude', 'magnitude'], axis=1))\n",
    "\n",
    "# Combine all processed data\n",
    "if all_data:\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"Merged DataFrame shape:\", merged_df.shape)\n",
    "else:\n",
    "    print(\"No CSV files found.\")\n",
    "\n",
    "train_pipeline(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24181</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>9.04</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24208</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>9.60</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24235</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>10.29</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24261</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>10.24</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24283</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>9.80</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>climbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300415</th>\n",
       "      <td>269668</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300416</th>\n",
       "      <td>269689</td>\n",
       "      <td>-2.57</td>\n",
       "      <td>8.96</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300417</th>\n",
       "      <td>269712</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>9.20</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300418</th>\n",
       "      <td>269735</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>9.42</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300419</th>\n",
       "      <td>269756</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>9.24</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time     x      y     z     label\n",
       "0        24181 -1.86   9.04 -1.62  climbing\n",
       "1        24208 -1.33   9.60 -2.19  climbing\n",
       "2        24235 -1.32  10.29 -2.32  climbing\n",
       "3        24261 -1.81  10.24 -2.96  climbing\n",
       "4        24283 -1.85   9.80 -3.04  climbing\n",
       "...        ...   ...    ...   ...       ...\n",
       "300415  269668 -2.74   8.79 -0.99   walking\n",
       "300416  269689 -2.57   8.96 -1.08   walking\n",
       "300417  269712 -2.03   9.20 -1.05   walking\n",
       "300418  269735 -2.54   9.42 -1.11   walking\n",
       "300419  269756 -2.83   9.24 -1.23   walking\n",
       "\n",
       "[300420 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
